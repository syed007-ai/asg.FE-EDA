{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6efc515-fca6-4e28-a0cc-cfca9b4c703f",
   "metadata": {},
   "source": [
    "# 1. What is the Filter method in feature selection, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69109dbd-7136-468c-94e4-089c0dd38a9c",
   "metadata": {},
   "source": [
    "- The Filter method in feature selection is like having a checklist of criteria to decide which features (attributes) are important for our machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9af22562-6f2f-4b65-98b3-b5b8875dd65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100   1   0   8]\n",
      " [150   0   1   9]\n",
      " [120   1   0   7]\n",
      " [ 90   0   0   5]\n",
      " [ 95   0   1   9]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.feature_selection import VarianceThreshold \n",
    "\n",
    "## sample data, with one hot encoded for the \"color\" feature-  \n",
    "# ** > Color': ['Red', 'Yellow', 'Red', 'Green', 'Yellow']  to  ** \n",
    "# 'Color_Red': [1, 0, 1, 0, 0],\n",
    "# 'Color_Yellow': [0, 1, 0, 0, 1],\n",
    "## To use the Filter method with categorical data, \n",
    "## we need to preprocess the categorical variables into numerical form.\n",
    "## One common technique for this is one-hot encoding. \n",
    "\n",
    "data = {'Weight': [100, 150, 120, 90, 95],\n",
    "        'Color_Red': [1, 0, 1, 0, 0],\n",
    "        'Color_Yellow': [0, 1, 0, 0, 1],\n",
    "        'Sweetness': [8, 9, 7, 5, 9]}\n",
    "# we've one-hot encoded the 'Color' feature, which converts it into binary (0 or 1) columns.\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "## Separate features \n",
    "X = df[[\"Weight\", \"Color_Red\",\"Color_Yellow\", \"Sweetness\"]]\n",
    "\n",
    "## Applying the the filter method using Variance Threshold to remove low-variance features \n",
    "## The VarianceThreshold method works with numerical data. \n",
    "selector = VarianceThreshold(threshold= 0.1) ## Adjust the threshold as needed \n",
    "X_filtered = selector.fit_transform(X)\n",
    "\n",
    "print(X_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c03147-1382-4610-88c9-6ed9cfa6d8d1",
   "metadata": {},
   "source": [
    "- In this example, we have data about different fruits, and we want to select the most relevant features for a model. \n",
    "- The filter method, in this case, uses the VarianceThreshold function to check the variance (how much values vary) of each feature.\n",
    "- If a feature doesn't vary much (e.g., all fruits have the same color), it might not be very informative.\n",
    "\n",
    "> - Hence, by applying the filter method, we can automatically remove features with low variance and keep only those that provide more diverse and valuable information. \n",
    "> - It's like decluttering our dataset to focus on the characteristics that matter most for our model, such as weight and sweetness in the case of fruit classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9f6540-36cf-4ce9-a785-b58f42e99505",
   "metadata": {},
   "source": [
    "- The Filter method in feature selection is like using a checklist to pick the best ingredients for a recipe.\n",
    "- In machine learning, it helps choose the most important features (like ingredients) for our model, so it can make better predictions.\n",
    "> - > Consider, we have a dataset with many features (like weight, color, and sweetness for fruits).\n",
    "> - > The Filter method checks each feature's characteristics. \n",
    "> - > For example, it may look at how much each feature varies (like how much the sweetness of fruits varies).\n",
    "> - > If a feature doesn't vary much or doesn't seem to affect the outcome (like if all fruits have the same color), the Filter method may say, \"This feature isn't that important.\n",
    "> - > So, it removes those less important features, leaving you with the key ingredients (important features) for your recipe (machine learning model).\n",
    "- Hence, if you're trying to predict if a fruit is sweet based on weight, color, and sweetness, the Filter method might tell you that color isn't very important. It's like using only the most critical ingredients to make your dish taste just right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc810e0-2db7-485d-8082-84e80f72215e",
   "metadata": {},
   "source": [
    "# 2. How does the Wrapper method differ from the Filter method in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18317358-21d9-4571-ad3d-9927ffa9ff12",
   "metadata": {},
   "source": [
    "- The **Wrapper** method is like trying out different outfits and asking for opinions to find the best one.\n",
    "- It involves building and testing several models using different subsets of features and selecting the one that performs the best.\n",
    "\n",
    "> - Imagine you want to decide what clothes to wear for a special occasion. \n",
    "> - You try different outfits, ask friends for their opinions, and choose the one that gets the most compliments.\n",
    "\n",
    "- The Filter method is like having a checklist of characteristics to decide which ingredients are essential for a recipe.\n",
    "- It uses statistical measures to evaluate individual features and select them based on their properties.\n",
    "\n",
    "> -  When you're cooking, you have a list of necessary ingredients like salt, pepper, and sugar.\n",
    "> -  You don't try different combinations; you choose ingredients based on their individual importance.\n",
    "\n",
    "> - > Therefore, the Wrapper method is like trying out different outfits (feature subsets) to find the best one, while the Filter method is like selecting ingredients (features) based on their individual characteristics.\n",
    "> - > Both methods have their uses depending on the situation, just as you might choose different methods for deciding what to wear or what ingredients to use in your cooking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "779911c0-6b1f-4067-b6eb-f2772e7bcab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best features: ['Weight', 'Color', 'Shape']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data: -> PS : Predicting if a fruit is sweet based on weight, color, and shape.\n",
    "data = {\n",
    "    'Weight': [100, 150, 120, 90, 95],\n",
    "    'Color': ['Red', 'Yellow', 'Red', 'Green', 'Yellow'],\n",
    "    'Shape': ['Round', 'Oval', 'Round', 'Oval', 'Round'],\n",
    "    'Sweetness': [8, 9, 7, 5, 9]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Mapping for converting categorical values to numerical representations\n",
    "color_mapping = {'Red': 0, 'Yellow': 1, 'Green': 2}\n",
    "shape_mapping = {'Round': 0, 'Oval': 1}\n",
    "\n",
    "# Converting categorical values to numerical representations\n",
    "df['Color'] = df['Color'].map(color_mapping)\n",
    "df['Shape'] = df['Shape'].map(shape_mapping)\n",
    "\n",
    "X = df[['Weight', 'Color', 'Shape']]\n",
    "y = df['Sweetness']\n",
    "\n",
    "# Defining a simple scoring function for evaluating feature subsets\n",
    "def score_features(features):\n",
    "    # Here, we can use a machine learning model to evaluate performance,\n",
    "    # but for this example, we'll calculate a simple score based on the sum.\n",
    "    return df[features].sum().sum()\n",
    "\n",
    "# Initialize variables to keep track of the best feature subset and score\n",
    "best_features = []\n",
    "best_score = 0\n",
    "\n",
    "# Trying different feature subsets\n",
    "for feature_subset in [['Weight'], ['Weight', 'Color'], ['Weight', 'Shape'], ['Weight', 'Color', 'Shape']]:\n",
    "    score = score_features(feature_subset)\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_features = feature_subset\n",
    "\n",
    "print(f'Best features: {best_features}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc208455-e4c8-4975-95fa-f849ef447187",
   "metadata": {},
   "source": [
    "> - > We've created a scoring function that calculates a simple score based on the sum of the selected features.\n",
    "> - > In a real-world scenario, we would use a machine learning algorithm to assess the quality of different feature subsets, and the Wrapper method would involve training and evaluating models with different feature combinations to select the best subset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7fb2fb-dc1f-41ae-86b7-31c483f53c7a",
   "metadata": {},
   "source": [
    "# 3. What are some common techniques used in Embedded feature selection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913d63a6-227e-462c-9c8c-79c9ddd25156",
   "metadata": {},
   "source": [
    "- Embedded feature selection methods are like having a smart chef who not only cooks a delicious meal but also decides which ingredients are the best. These methods are integrated into the machine learning model training process. Here are some common techniques:\n",
    "\n",
    "# **[i]. L1 Regularization (Lasso):** It's like using a magnifying glass to focus on the most important ingredients.\n",
    "- L1 regularization encourages the model to set some feature coefficients to zero, effectively eliminating less important features.\n",
    "# **[ii]. Tree-Based Methods:** Imagine sorting ingredients by their importance on a recipe.\n",
    "- Decision trees and random forests can do this naturally. \n",
    "- Features that contribute more to splitting and decision-making are considered more important.\n",
    "# **[iii]. Gradient Boosting:** It's like refining a recipe by repeatedly asking experts (weak models) and combining their opinions.\n",
    "- Gradient boosting assigns importance to features based on how much they help improve predictions during the boosting process.\n",
    "# **[iv]. Feature Importance Scores:** Think of it as scoring ingredients based on their contribution to the dish's taste. \n",
    "- Different models like Random Forest and XGBoost provide feature importance scores, which tell you which features are most influential.\n",
    "# **[v]. Recursive Feature Elimination:** It's like gradually removing ingredients that don't add much flavor. \n",
    "- This technique starts with all features and iteratively eliminates the least important ones based on model performance.\n",
    "# **[vi]. Embedded Neural Networks:** In deep learning, it's like having a chef network that learns to use the best ingredients.\n",
    "- Neural networks can adaptively adjust feature importance during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08180734-5fa3-49fc-a239-21ce50263b74",
   "metadata": {},
   "source": [
    "> - > * Embedded methods make your model smarter by automatically selecting the most relevant ingredients (features), ensuring it cooks up the tastiest predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81913f0-7709-4c15-80e3-dd581e56f1ac",
   "metadata": {},
   "source": [
    "# 4. What are some drawbacks of using the Filter method for feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d06e8cb-8f1e-47d5-affc-4260160029cd",
   "metadata": {},
   "source": [
    "- Using the Filter method for feature selection is like sorting out ingredients before cooking, but it has some drawbacks:\n",
    "\n",
    "# **- Ignores Interaction:** It doesn't consider how ingredients work together.\n",
    "- It evaluates features individually, so it may miss valuable combinations that enhance the dish's flavor.\n",
    "# **- May Remove Important Ingredients:** The Filter method doesn't account for how crucial a feature is in the context of the entire recipe.\n",
    "- It might discard a less popular ingredient that's vital for the final taste.\n",
    "# **- Doesn't Adapt:** It's like sticking to one recipe for all occasions. \n",
    "- The Filter method doesn't adjust to different dishes (datasets), so it may not be suitable for every cooking scenario.\n",
    "# **- Doesn't Use Learning:** Like not learning from the taste test, the Filter method doesn't incorporate feedback from the model. \n",
    "- It doesn't improve its feature selection as the model learns.\n",
    "# **- Limited for Complex Recipes:** When making intricate dishes (complex models), the Filter method's basic checklist might not capture all the nuances of the recipe, potentially leading to bland results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e55208b-96b7-4119-b660-d214d62580db",
   "metadata": {},
   "source": [
    "> - > * The Filter method simplifies feature selection, but it may not be the best choice for every culinary challenge (machine learning problem). It's essential to choose the right technique that suits the specific recipe (dataset) and cooking style (model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44cecbb-b641-4810-826a-18c978f5dde2",
   "metadata": {},
   "source": [
    "# 5. In which situations would you prefer using the Filter method over the Wrapper method for feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb6ac87-7d55-4beb-ad93-af6e249521dd",
   "metadata": {},
   "source": [
    "- We might prefer using the Filter method for feature selection in these situations:\n",
    "\n",
    "# **-When we Have a Large Dataset:** If we have a massive recipe book (dataset) with many ingredients (features), the Filter method is quicker and more efficient because it doesn't involve extensive model training.\n",
    "# **-For Quick Preprocessing:** When we need a fast way to weed out obviously irrelevant ingredients (features) before diving into more complex cooking (model training), the Filter method is like a simple ingredient checklist.\n",
    "# **-For Initial Exploration:** If we're just starting to experiment with a new dish (machine learning problem), the Filter method can give us a basic understanding of which ingredients are likely to be important without spending too much time.\n",
    "# **-When Interactions Are Less Important:** If your dish (model) relies more on the individual flavors of ingredients (features) rather than how they interact, the Filter method can be sufficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4069768-7b04-430e-8494-10a186b7a86b",
   "metadata": {},
   "source": [
    "> - > * Hence Filter method acts like a quick kitchen tool for a preliminary ingredient selection. \n",
    "> - > * However, for more nuanced and complex recipes (machine learning problems), the Wrapper method, which actively involves the model in the selection process, might be a better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777a9efb-3cc5-44a7-a4ac-05087aeb249e",
   "metadata": {},
   "source": [
    "# 6. In a telecom company, you are working on a project to develop a predictive model for customer churn. You are unsure of which features to include in the model because the dataset contains several different ones. Describe how you would choose the most pertinent attributes for the model using the Filter Method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9fc71c-bf79-4eae-80ff-271bd25ad95f",
   "metadata": {},
   "source": [
    "- In a telecom company's customer churn project, using the Filter Method for feature selection is like having a checklist to choose the most relevant factors that impact customer churn. Here's how I would go about it:\n",
    "\n",
    "# - [i] Collect Data: Gather a dataset with information about telecom customers, such as call duration, monthly charges, contract type, customer feedback, and more.\n",
    "# - [ii] Feature Selection: Now, we have many attributes (features). To decide which ones to include in your churn prediction model, we apply the Filter Method.\n",
    "# - [iii] Check Feature Characteristics: We start by examining each feature's characteristics. For example, we may calculate the statistical significance of features' relationships with customer churn. Features that show a strong connection with churn are more likely to be relevant.\n",
    "# - [iv] Use Metrics: We may use metrics like correlation coefficients, chi-squared tests, or information gain to assess how well each feature explains customer churn. Features with high scores are considered more pertinent.\n",
    "# - [v] Select the Best Features: Based on these metrics, we create a checklist of the most relevant features. These are the ingredients we believe will have the most impact on predicting customer churn.\n",
    "# - [vi] Model Building: With your selected features, we can now build a predictive model for customer churn. These features are like the key ingredients that will help our model make accurate predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52496d85-1a49-4a98-a351-7360a74136eb",
   "metadata": {},
   "source": [
    "> - > * The Filter Method simplifies the process by allowing us to quickly narrow down your list of ingredients (features) to the most promising ones, ensuring that our churn prediction model is focused on the factors that matter the most."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fc449e-18c0-4f75-92dd-1167f5e69e3b",
   "metadata": {},
   "source": [
    "# 7. You are working on a project to predict the outcome of a soccer match. You have a large dataset with many features, including player statistics and team rankings. Explain how you would use the Embedded method to select the most relevant features for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba761dc2-3df7-468d-9493-cc62e26d62f7",
   "metadata": {},
   "source": [
    "- In your soccer match prediction project, using the Embedded Method for feature selection is like having a soccer coach who not only trains the team but also decides which players are the best for the upcoming match. Here's how I would do it:\n",
    "\n",
    "# - [i] Collect Data: We gather a large dataset with lots of information about soccer matches, including player statistics, team rankings, and various other factors that might affect the outcome.\n",
    "# - [ii] Feature Engineering: Now, we have many features, and some of them might be related to each other. We transform and engineer these features to make them more useful for predicting match outcomes. For example, we might calculate team performance averages or player-to-team compatibility scores.\n",
    "# - [iii] Model Training: We create a predictive model to forecast match results based on the available features. Think of this as training your soccer team to perform well.\n",
    "# - [iv] Feature Importance: During the model training process, the model evaluates the importance of each feature. It's like the coach assessing the performance of each player during training sessions and matches.\n",
    "# - [v] Iterative Learning: The model adapts and learns which features are the most valuable by continuously training and evaluating itself. Just like a coach refining the lineup for each match based on players' performance in previous games.\n",
    "# - [vi] Select the Best Features: The Embedded Method automatically selects the most relevant features by considering how they impact the model's performance. These are the players (features) that contribute the most to predicting match outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978b4fd7-8139-4e87-a80b-8b79882e2f7e",
   "metadata": {},
   "source": [
    "> - > - Therefore, with the Embedded Method, we're not only building a prediction model but also allowing it to decide which features (players) are the best fit for winning the soccer match. It's like having an adaptive coach and team that improve over time to ensure accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f47e70a-8e44-419d-928a-40e227eb483b",
   "metadata": {},
   "source": [
    "# 8. You are working on a project to predict the price of a house based on its features, such as size, location, and age. You have a limited number of features, and you want to ensure that you select the most important ones for the model. Explain how you would use the Wrapper method to select the best set of features for the predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5257760-cf4a-40d3-b823-737c2895399c",
   "metadata": {},
   "source": [
    "- In our project to predict house prices, using the Wrapper Method for feature selection is like finding the perfect combination of key ingredients to bake the best cake. Here's how we would go about it:\n",
    "\n",
    "# - [i] Create a Recipe (Model): We start by setting up your recipe, which is our prediction model. This model will use different combinations of ingredients (features) to predict house prices.\n",
    "# - [ii] Gather Ingredients (Features): We collect all the available features, such as house size, location, age, and any others that might influence the price. These are like the ingredients for your cake.\n",
    "# - [iii] Bake a Test Cake (Model Training): We begin by baking a test cake (training your model) using all the available ingredients (features). This initial cake might be decent, but it could be improved.\n",
    "# - [iv] Taste the Cake (Evaluate Model): We taste the cake (evaluate the model's performance) and see how well it predicts house prices. If it's not perfect, it means some ingredients (features) might be unnecessary or missing.\n",
    "# - [v] Try Different Ingredients (Feature Combinations): Now, we start experimenting with different combinations of ingredients (features). We might try one cake with just size and location, another with age and location, and so on.\n",
    "# - [vi] Taste Each Cake (Evaluate Model Again): For each new cake (model with a different feature combination), we taste it (evaluate the model) and see how well it predicts house prices.\n",
    "# - [vii] Select the Best Cake (Feature Set): Finally, we choose the cake (feature combination) that tastes the best, meaning it results in the most accurate predictions.These selected features are like the perfect combination of ingredients for our house price prediction model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d17b972-3687-47e9-b65b-9ee8d06da045",
   "metadata": {},
   "source": [
    "> - > - By using the Wrapper Method, we're essentially finding the ideal blend of features to make our prediction model perform at its best, just like discovering the perfect ingredients for baking the tastiest cake."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
