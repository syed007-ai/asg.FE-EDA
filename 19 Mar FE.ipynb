{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a09fc947-41be-4002-9125-049d0877b989",
   "metadata": {},
   "source": [
    "# 1.  What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d86b45-9b9e-4f49-9c20-a730137a5e27",
   "metadata": {},
   "source": [
    "- Min-Max scaling is like adjusting the volume of your music to make it fit the mood of the party.\n",
    "- In data preprocessing, it helps to transform data values into a specific range, typically between 0 and 1, so they play well together and don't overshadow each other.\n",
    "\n",
    "> - Here's how it works!! :\n",
    "\n",
    "> - >  [i] **Find the Minimum and Maximum Values:** Imagine you have a playlist, and you want to know the softest and loudest songs. \n",
    "> - > You find the quietest (minimum) and the loudest (maximum) song volumes.\n",
    "> - >  [ii] **Rescale the Values:** Now, you adjust all song volumes so they fit between 0 (quietest) and 1 (loudest). \n",
    "> - > It's like turning the volume knob in a way that soft songs become 0, loud songs become 1, and everything else adjusts proportionally in between.\n",
    "\n",
    "> - eg: \n",
    "- Let's say you have a dataset of house sizes, and they range from 500 square feet (the smallest) to 2000 square feet (the largest).\n",
    "\n",
    "- A 500-square-foot house would be scaled to 0.\n",
    "- A 1000-square-foot house would be scaled to 0.5 (right in the middle).\n",
    "- A 2000-square-foot house would be scaled to 1 (the largest). \n",
    "\n",
    "> Hence, Min-Max scaling helps to put all the house sizes on the same volume level, making it easier for a model to understand and work with the data. \n",
    "> - It's like making sure all songs in your playlist have a similar volume so that no single song drowns out the others at your party."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dd21394-9599-447a-b726-8a59782af94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   House Size (sq. ft.)  Scaled Size\n",
      "0                   500     0.000000\n",
      "1                  1000     0.333333\n",
      "2                  1500     0.666667\n",
      "3                  2000     1.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# sample data \n",
    "data = {\n",
    "    'House Size (sq. ft.)': [500, 1000, 1500, 2000] # House sizes in square feet\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initializing the Min-Max scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fit and transform the data to scale it between 0 and 1\n",
    "df['Scaled Size'] = scaler.fit_transform(df[['House Size (sq. ft.)']])\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e43c73-c555-4be5-8ee0-3201d4fdfe88",
   "metadata": {},
   "source": [
    "- Here, we first created a dataset with house sizes. \n",
    "- Then, we used the Min-Max scaler from scikit-learn to transform the data. \n",
    "- The \"Scaled Size\" column represents the Min-Max scaled values, where 0 corresponds to the smallest house size, and 1 corresponds to the largest house size.\n",
    "- All house sizes in between are scaled proportionally within this range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d275bb8-680b-4fbf-9e76-aeb4df820a3e",
   "metadata": {},
   "source": [
    "# 2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c24c87-74c3-4782-9381-59d6ab15031f",
   "metadata": {},
   "source": [
    "- The **Unit Vector technique** in feature scaling is like resizing all the arrows in your collection to have the same length without changing their directions.\n",
    "- It ensures that every feature has the same influence (length) in your data, making them equally important, just like each arrow has the same length when measured.\n",
    "\n",
    "> - Here's how it differs from Min-Max scaling!! :\n",
    "\n",
    "> - > - [i] **Min-Max Scaling:** It resizes the arrows (features) to fit within a specific range (e.g., between 0 and 1). \n",
    "- It adjusts the lengths to play well within that range, like adjusting the volume of songs in a playlist to fit between the quietest and loudest.\n",
    "> - > - [ii] **Unit Vector Scaling:** Instead of fitting the arrows within a range, it changes their lengths so that they all have a length of 1.\n",
    "- The direction (relationship) between them remains the same, but their lengths are adjusted so they all have equal influence.\n",
    "\n",
    "> - eg : \n",
    "\n",
    "- Let's use a dataset of houses with two features: \"House Size\" and \"Number of Bedrooms.\n",
    "- \" Min-Max scaling would fit these features within a specific range, like resizing two arrows to have lengths between 0 and 1.\n",
    "- Unit Vector scaling, on the other hand, would make both arrows have a length of 1 while preserving their direction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74f3872f-5e08-434d-905d-c0ffa1f49cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   House Size (sq. ft.)  Number of Bedrooms  Scaled Size  Scaled Bedrooms\n",
      "0                   500                   1     0.999998            0.002\n",
      "1                  1000                   2     0.999998            0.002\n",
      "2                  1500                   3     0.999998            0.002\n",
      "3                  2000                   4     0.999998            0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/base.py:402: UserWarning: X has feature names, but Normalizer was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# sample data\n",
    "data = {\n",
    "    'House Size (sq. ft.)': [500, 1000, 1500, 2000],  # House size and number of bedrooms\n",
    "    'Number of Bedrooms': [1, 2, 3, 4]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# initializing the Normalizer for Unit Vector scaling\n",
    "normalizer = Normalizer()\n",
    "\n",
    "# fit and transform the data to create unit vectors\n",
    "df[['Scaled Size', 'Scaled Bedrooms']] = normalizer.transform(df)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5096c7d5-9729-4d58-ab18-60327cae1f36",
   "metadata": {},
   "source": [
    "- Here, we used the Normalizer from scikit-learn to apply Unit Vector scaling to the dataset.\n",
    "- The \"Scaled Size\" and \"Scaled Bedrooms\" columns represent the unit vectors, where each feature's length is 1, ensuring they have equal influence while maintaining their relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeaafad-0257-4b18-8143-31ddd8f83e7b",
   "metadata": {},
   "source": [
    "# 3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279e17e5-50f1-4bc6-b78b-40e7fcc03516",
   "metadata": {},
   "source": [
    "- **Principal Component Analysis (PCA)** is like simplifying a complex dish to its core flavors. \n",
    "- In dimensionality reduction, it helps you identify the most essential ingredients (features) in your data, allowing you to reduce the complexity while retaining the most critical information.\n",
    "\n",
    "> Here's how PCA works!! :\n",
    "\n",
    "> - > - [i] **Data Preparation:** Imagine you have a recipe with many ingredients, but some might be redundant or less important. \n",
    "- PCA helps you find the key ingredients (features) in your recipe.\n",
    "> - > - [ii] **Create New Ingredients:** PCA combines and transforms your original ingredients into new ones, known as \"principal components.\" \n",
    "- These components capture the most significant variations in your data. It's like creating flavor combinations to represent the core taste of a dish.\n",
    "> - > - [iii] **Rank by Importance:** PCA ranks these new components by their importance. \n",
    "- The first component represents the most significant variation in your data, the second represents the second most significant, and so on.\n",
    "> - > - [iv] **Choose Components:** You can decide how many principal components to keep, simplifying your recipe.\n",
    "- It's like selecting the primary flavors to make your dish without losing its essence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf906694-a4dd-47b9-9590-c662136ff1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Component 1  Component 2\n",
      "0     2.324567    -0.310461\n",
      "1     0.673887     0.214186\n",
      "2    -0.976793     0.738834\n",
      "3    -2.021662    -0.642559\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# sample data\n",
    "data = {\n",
    "    'Saltiness': [2, 3, 4, 5],  # Ingredients (features) in a dish\n",
    "    'Spiciness': [1, 2, 3, 4],\n",
    "    'Sweetness': [3, 2, 1, 2] \n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# initializing PCA with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# fit and transform the data to create principal components\n",
    "principal_components = pca.fit_transform(df)\n",
    "\n",
    "# creating a DataFrame to show the transformed data\n",
    "df_pca = pd.DataFrame(data=principal_components, columns=['Component 1', 'Component 2'])\n",
    "\n",
    "print(df_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2dcb83-7ec7-4690-be3b-c9692ae20601",
   "metadata": {},
   "source": [
    "- Here, we applied PCA to reduce the number of features (ingredients) while retaining the core information.\n",
    "- The resulting \"Component 1\" and \"Component 2\" represent the simplified, essential flavors of the dish."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8156f5fe-3966-45c8-80cd-f67840cb0fcf",
   "metadata": {},
   "source": [
    "# 4.  What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a245ac0b-78e1-4570-bd0e-32d4eda5dcb6",
   "metadata": {},
   "source": [
    "- PCA (Principal Component Analysis) and Feature Extraction are like two chefs working together to create a simplified and tastier dish.\n",
    "- In the world of data and machine learning, PCA is a technique used for feature extraction, which helps reduce the number of features (ingredients) while preserving the most important information.\n",
    "\n",
    "> - Here's how PCA can be used for feature extraction!! :\n",
    "\n",
    "> - > - [i] **Data Complexity Reduction:** Imagine you have a recipe with lots of ingredients, some of which are redundant or less important.\n",
    "- Feature extraction, with the help of PCA, simplifies the recipe by selecting only the key ingredients (features).\n",
    "> - > - [ii] **Create Essential Components:** PCA transforms the original ingredients into new components that capture the most significant variations in your data.\n",
    "- These components are like the core flavors of the dish.\n",
    "> - > - [iii] **Feature Reduction:** You can choose how many of these components to keep. \n",
    "- For example, if your original recipe had 20 ingredients (features), PCA might find that you can achieve the same delicious taste with just 5 components (features).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d2edcde-99af-490d-90a7-00420746abad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature 1  Feature 2\n",
      "0   2.324567  -0.310461\n",
      "1   0.673887   0.214186\n",
      "2  -0.976793   0.738834\n",
      "3  -2.021662  -0.642559\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# sample data\n",
    "data = {\n",
    "    'Saltiness': [2, 3, 4, 5], # Ingredients (features) in a dish\n",
    "    'Spiciness': [1, 2, 3, 4],\n",
    "    'Sweetness': [3, 2, 1, 2]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# initializing PCA to reduce to 2 components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# fit and transform the data to create principal components\n",
    "extracted_features = pca.fit_transform(df)\n",
    "\n",
    "# creating a DataFrame to show the extracted features\n",
    "df_extracted = pd.DataFrame(data=extracted_features, columns=['Feature 1', 'Feature 2'])\n",
    "\n",
    "print(df_extracted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c6dd35-983b-41de-ada5-30c60b2460f8",
   "metadata": {},
   "source": [
    "- Here, we applied PCA for feature extraction. \n",
    "- The \"Feature 1\" and \"Feature 2\" are the essential components that capture the most critical information from the original ingredients (features).\n",
    "- By using only these extracted features, we simplify our recipe without losing the core flavors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6feace4-a4ef-4631-aed3-b3e5c2642137",
   "metadata": {},
   "source": [
    "# 5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158dd458-0192-42a5-a1f5-776052f191bb",
   "metadata": {},
   "source": [
    "- In our food delivery recommendation system project, using Min-Max scaling is like making sure all your food preferences (features like price, rating, and delivery time) are on the same scale, just like you'd compare pizza prices, burger ratings, and sushi delivery times equally.\n",
    "\n",
    "> - Here's how we'd use Min-Max scaling!! :\n",
    "\n",
    "> - > - [i] **Data Gathering:** You collect a dataset with various features like food prices (ranging from $5 to $30), ratings (on a scale from 1 to 5), and delivery times (from 10 to 60 minutes).\n",
    "> - > - [ii] **Scaling Range:** You decide on a range, often between 0 and 1, in which you want to fit your data. \n",
    "- It's like saying you want to measure everything on a scale from \"not important\" (0) to \"very important\" (1).\n",
    "> - > - [iii] **Adjusting Values:** You apply Min-Max scaling to each feature individually.\n",
    "- It's like taking each feature's values and adjusting them proportionally so that they fit within your chosen range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e1debbb-4099-43d0-a79c-9543bf0ecd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Scaled Price  Scaled Rating  Scaled Delivery Time\n",
      "0           0.0       0.000000                   0.0\n",
      "1           0.4       0.666667                   0.4\n",
      "2           1.0       1.000000                   1.0\n",
      "3           0.2       0.333333                   0.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# sample data\n",
    "data = {\n",
    "    'Price': [5, 15, 30, 10],   #  food preferences with different scales\n",
    "    'Rating': [2, 4, 5, 3],\n",
    "    'Delivery Time (min)': [10, 30, 60, 20]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# initializing the Min-Max scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fit and transform the data to scale within the range [0, 1]\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# creating a DataFrame to show the scaled data\n",
    "df_scaled = pd.DataFrame(data=scaled_data, columns=['Scaled Price', 'Scaled Rating', 'Scaled Delivery Time'])\n",
    "\n",
    "print(df_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1e22a8-e8b5-4feb-b07c-999048e277e0",
   "metadata": {},
   "source": [
    "- Here, we applied Min-Max scaling to the dataset.\n",
    "- Which transformed the original data into a new scale where each feature is adjusted to fit within the range [0, 1].\n",
    "- Now, we can easily compare the scaled features, making it suitable for building our food delivery recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0eb8b4-661c-4e67-9ee8-5734b290cf2c",
   "metadata": {},
   "source": [
    "# 6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30cdf15-bc09-463f-ba85-63d0a59d0ca8",
   "metadata": {},
   "source": [
    "- In our stock price prediction project, using PCA (Principal Component Analysis) to reduce dimensionality is like simplifying a complex stock market analysis by focusing on the most critical factors, similar to distilling a vast recipe down to its essential ingredients.\n",
    "\n",
    "> - Here's how we'd use PCA for dimensionality reduction!! :\n",
    "\n",
    "> - > - [i] **Data Complexity Reduction:** Imagine we have a recipe with lots of ingredients (features), but not all of them are equally important. \n",
    "- Similarly, in our dataset, some features might be redundant or have less impact on stock prices.\n",
    "> - > - [ii] **Create New Factors:** PCA transforms our original features into new components (factors) that represent the most significant variations in our data. \n",
    "- These components are like the core flavors in our recipe.\n",
    "> - > - [iii] **Rank by Importance:** PCA ranks these new components based on their importance.\n",
    "- The first component captures the most significant variation, the second captures the second most, and so on.\n",
    "> - > - [iv] **Select Fewer Components:** We can choose how many of these components to keep. \n",
    "- Instead of dealing with all the original features, we focus on a smaller set of the most influential components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ed2be88-a36a-42f1-992f-9098ee327654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Component 1  Component 2\n",
      "0    -0.051828    -0.006746\n",
      "1     0.252880    -0.007247\n",
      "2    -0.252863    -0.002488\n",
      "3     0.051811     0.016481\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# sample data\n",
    "data = {\n",
    "    'Earnings Growth': [0.05, 0.03, 0.06, 0.02],    # company financial data and market trends (features)\n",
    "    'Market Volatility': [0.1, 0.15, 0.08, 0.12],\n",
    "    'Debt-to-Equity Ratio': [1.2, 0.9, 1.4, 1.1],\n",
    "    # ... More financial and market features\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# initializing PCA to reduce to 2 components (for simplicity)\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# fit and transform the data to create principal components\n",
    "reduced_features = pca.fit_transform(df)\n",
    "\n",
    "# creating a DataFrame to show the reduced features\n",
    "df_reduced = pd.DataFrame(data=reduced_features, columns=['Component 1', 'Component 2'])\n",
    "\n",
    "print(df_reduced)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e97413-9f0d-475e-9b5a-6f4b0dfcddcd",
   "metadata": {},
   "source": [
    "- Here, we applied PCA to reduce the dimensionality of the dataset. \n",
    "- The \"Component 1\" and \"Component 2\" represent the most essential components that capture the primary variations in the data.\n",
    "- By focusing on these components, we simplify our analysis while retaining the core factors that influence stock prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47529695-34d5-43db-9b93-ac1b83d6eaff",
   "metadata": {},
   "source": [
    "# 7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75529aa6-59fe-4148-af7a-6b3e1ee62452",
   "metadata": {},
   "source": [
    "- Min-Max scaling is like resizing a group of values so they fit within a specific range, and in this case, we want to make them fit between -1 and 1. \n",
    "- It's similar to adjusting the volume of music to make it neither too quiet (less than -1) nor too loud (more than 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72f8608a-6e7f-4784-9c55-fefb8839fb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0, -0.5789473684210527, -0.052631578947368474, 0.4736842105263157, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# sample data\n",
    "data = [1, 5, 10, 15, 20]\n",
    "\n",
    "# defining the minimum and maximum values in the original data\n",
    "min_value = min(data)\n",
    "max_value = max(data)\n",
    "\n",
    "# defining the desired range (from -1 to 1)\n",
    "new_min = -1\n",
    "new_max = 1\n",
    "\n",
    "# performing Min-Max scaling\n",
    "scaled_data = [((x - min_value) / (max_value - min_value)) * (new_max - new_min) + new_min for x in data]\n",
    "\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3738c02d-6048-47a9-b6bb-532f81e3947f",
   "metadata": {},
   "source": [
    "- Here, we first find the minimum and maximum values in the original data.\n",
    "- Then, we use these values to transform the data into the desired range of -1 to 1.\n",
    "- The scaled data will now be adjusted to fit within this range, making it easier to compare and work with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb36ce0-a096-4622-99eb-96a879b8aad5",
   "metadata": {},
   "source": [
    "# 8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab546f0c-17c5-40f8-9353-1dae8d4ec286",
   "metadata": {},
   "source": [
    "- In feature extraction using PCA, we aim to simplify our dataset by focusing on the most essential factors while reducing complexity.\n",
    "- The number of principal components to retain depends on how much variance (variation) in our data we want to preserve.\n",
    "- We typically choose to keep a sufficient number of principal components to maintain a high percentage of the original data's variance.\n",
    "- A common choice is to retain components that capture 95% or more of the variance.\n",
    "\n",
    "> - eg : \n",
    "- If we choose to retain 2 principal components, it means we keep the two most influential factors that explain a significant portion of the data's variance.\n",
    "- If we choose to retain 3 principal components, it means we keep three factors that explain even more of the data's variance.\n",
    "\n",
    "\n",
    "- > - The decision on how many principal components to retain depends on the trade-off between simplification and preserving data's information. Retaining more principal components can capture more variance but may not significantly reduce complexity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98bb5f80-71ae-46e5-b5d9-a64307c05390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of principal components to retain: 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# sample data\n",
    "data = {\n",
    "    'Height': [170, 180, 160, 165, 175], #  Features (height, weight, age, gender, blood pressure)\n",
    "    'Weight': [70, 90, 60, 65, 80],\n",
    "    'Age': [30, 40, 25, 35, 50],\n",
    "    'Gender': [0, 1, 0, 1, 0],  # Assuming 0 for male and 1 for female\n",
    "    'Blood Pressure': [120, 140, 110, 130, 135]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# initializing PCA with no specified number of components\n",
    "pca = PCA()\n",
    "\n",
    "# fit PCA on the data\n",
    "pca.fit(df)\n",
    "\n",
    "# determining how many components to retain to capture 95% of the variance\n",
    "cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
    "num_components = len(cumulative_variance[cumulative_variance < 0.95]) + 1\n",
    "\n",
    "print(f\"Number of principal components to retain: {num_components}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9351f36-a7eb-4a06-b8f5-b2dbe81b0c1c",
   "metadata": {},
   "source": [
    "- Here, we used PCA to determine how many principal components to retain to capture 95% of the data's variance. \n",
    "- The number of components we choose helps strike a balance between simplification and preserving data information"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
